{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/akash/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/akash/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/home/akash/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/akash/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/home/akash/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/home/akash/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "nlp = stanfordnlp.Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    }
   ],
   "source": [
    "txt = \"what is the other name of panipuri?\"\n",
    "doc = nlp(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what</td>\n",
       "      <td>WP</td>\n",
       "      <td>what</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>be</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>other</td>\n",
       "      <td>JJ</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>name</td>\n",
       "      <td>NN</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>panipuri</td>\n",
       "      <td>NN</td>\n",
       "      <td>panipuri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>?</td>\n",
       "      <td>.</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lemma  pos      word\n",
       "0      what   WP      what\n",
       "1        be  VBZ        is\n",
       "2       the   DT       the\n",
       "3     other   JJ     other\n",
       "4      name   NN      name\n",
       "5        of   IN        of\n",
       "6  panipuri   NN  panipuri\n",
       "7         ?    .         ?"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#extract lemma\n",
    "def extract_lemma(doc):\n",
    "    parsed_text = {'word':[], 'lemma':[], \"pos\":[]}\n",
    "    for sent in doc.sentences:\n",
    "        for wrd in sent.words:\n",
    "            #extract text and lemma\n",
    "            parsed_text['word'].append(wrd.text)\n",
    "            parsed_text['lemma'].append(wrd.lemma)\n",
    "            parsed_text['pos'].append(wrd.pos)\n",
    "    #return a dataframe\n",
    "    return pd.DataFrame(parsed_text)\n",
    "\n",
    "#call the function on doc\n",
    "extract_lemma(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('While', 'O'), ('in', 'O'), ('France', 'LOCATION'), (',', 'O'), ('Christine', 'PERSON'), ('Lagarde', 'PERSON'), ('discussed', 'O'), ('short-term', 'O'), ('stimulus', 'O'), ('efforts', 'O'), ('in', 'O'), ('a', 'O'), ('recent', 'O'), ('interview', 'O'), ('with', 'O'), ('the', 'O'), ('Wall', 'ORGANIZATION'), ('Street', 'ORGANIZATION'), ('Journal', 'ORGANIZATION'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "st = StanfordNERTagger('stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz','stanford-ner/stanford-ner.jar',encoding='utf-8')\n",
    "\n",
    "text = 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.'\n",
    "\n",
    "tokenized_text = word_tokenize(text)\n",
    "classified_text = st.tag(tokenized_text)\n",
    "\n",
    "print(classified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('While', 'IN'), ('in', 'IN'), ('France', 'NNP'), (',', ','), ('Christine', 'NNP'), ('Lagarde', 'NNP'), ('discussed', 'VBD'), ('short-term', 'JJ'), ('stimulus', 'NN'), ('efforts', 'NNS'), ('in', 'IN'), ('a', 'DT'), ('recent', 'JJ'), ('interview', 'NN'), ('with', 'IN'), ('the', 'DT'), ('Wall', 'NNP'), ('Street', 'NNP'), ('Journal', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag.stanford import StanfordPOSTagger as POS_Tag\n",
    "postagger = POS_Tag('postagger/models/english-bidirectional-distsim.tagger', 'postagger/stanford-postagger.jar')\n",
    "\n",
    "text = 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.'\n",
    "\n",
    "tokenized_text = word_tokenize(text)\n",
    "classified_text = postagger.tag(tokenized_text)\n",
    "\n",
    "print(classified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('While', 'O'), ('France', 'LOCATION'), ('Christine', 'PERSON'), ('Lagarde', 'PERSON'), ('discussed', 'O'), ('short-term', 'O'), ('stimulus', 'O'), ('efforts', 'O'), ('recent', 'O')]\n",
      "[('While', 'IN'), ('France', 'NNP'), ('Christine', 'NNP'), ('Lagarde', 'NNP'), ('discussed', 'VBD'), ('short-term', 'JJ'), ('stimulus', 'NN'), ('efforts', 'NNS'), ('recent', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tag.stanford import StanfordPOSTagger as POS_Tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stopword = set(stopwords.words('english'))\n",
    "stopword.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "\n",
    "st_NER_PATH1 = 'stanford-nlp/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "st_NER_PATH2 = 'stanford-nlp/stanford-ner/stanford-ner.jar'\n",
    "st_NER = StanfordNERTagger(st_NER_PATH1, st_NER_PATH2, encoding='utf-8')\n",
    "\n",
    "st_POS_PATH1 = 'stanford-nlp/postagger/models/english-bidirectional-distsim.tagger'\n",
    "st_POS_PATH2 = 'stanford-nlp/postagger/stanford-postagger.jar'\n",
    "st_POS = POS_Tag(st_POS_PATH1, st_POS_PATH2)\n",
    "\n",
    "\n",
    "text = 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent.'\n",
    "tokenized_text = [i for i in word_tokenize(text) if i not in stopword]\n",
    "\n",
    "ner_text = st_NER.tag(tokenized_text)\n",
    "pos_text = st_POS.tag(tokenized_text)\n",
    "\n",
    "print(ner_text)\n",
    "print(pos_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize: ['Dosa', 'famous']\n",
      "ner: [('Dosa', 'O'), ('famous', 'O')]\n",
      "POS: [('Dosa', 'NN'), ('famous', 'JJ')]\n",
      "qdetail: ['FACTOID', 'PLACE', ['Dosa', 'famous']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tag.stanford import StanfordPOSTagger as POS_Tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from qType import processquestion\n",
    "stopword = set(stopwords.words('english'))\n",
    "stopword.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "\n",
    "\n",
    "class QpreProcessing:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.st_NER_PATH1 = 'stanford-nlp/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "        self.st_NER_PATH2 = 'stanford-nlp/stanford-ner/stanford-ner.jar'\n",
    "        self.st_NER = StanfordNERTagger(self.st_NER_PATH1, self.st_NER_PATH2, encoding='utf-8')\n",
    "\n",
    "        self.st_POS_PATH1 = 'stanford-nlp/postagger/models/english-bidirectional-distsim.tagger'\n",
    "        self.st_POS_PATH2 = 'stanford-nlp/postagger/stanford-postagger.jar'\n",
    "        self.st_POS = POS_Tag(self.st_POS_PATH1, self.st_POS_PATH2)\n",
    "\n",
    "    \n",
    "    def keywords(self, sentence):\n",
    "        '''removes stop words too'''\n",
    "        doc = [i for i in word_tokenize(sentence) if i not in stopword]\n",
    "        return doc\n",
    "\n",
    "    def pos(self, sentence):\n",
    "        pos_text = self.st_POS.tag(self.keywords(sentence))\n",
    "        return pos_text\n",
    "    \n",
    "    def ner(self, sentence):\n",
    "        ner_text = self.st_NER.tag(self.keywords(sentence))\n",
    "        return ner_text\n",
    "    \n",
    "    def qType_words(self, sentence):\n",
    "        qwords  = word_tokenize(sentence)\n",
    "        (type, target) = processquestion(qwords)\n",
    "        if type=='YESNO':\n",
    "            return [type, target]\n",
    "        else:\n",
    "            qDetail = ['FACTOID']\n",
    "            qDetail=qDetail+[type, target]\n",
    "            return qDetail\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    sNLP = QpreProcessing()\n",
    "    text = 'where is Dosa famous'\n",
    "    tree1 = sNLP.keywords(text)\n",
    "    tree2 = sNLP.ner(text)\n",
    "    tree3 = sNLP.pos(text)\n",
    "    tree4 = sNLP.qType_words(text)\n",
    "    print(\"tokenize:\", tree1)\n",
    "    print(\"ner:\", tree2)\n",
    "    print(\"POS:\", tree3)\n",
    "    print(\"qdetail:\", tree4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/akash/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/akash/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/home/akash/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/akash/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/home/akash/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/home/akash/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize: ['he', 'will', 'be', 'her', 'in', 'china', '.']\n",
      "lemmatize: ['he', 'will', 'be', 'she', 'in', 'china', '.']\n",
      "POS: ['PRP', 'MD', 'VB', 'PRP', 'IN', 'NNP', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "''' help from https://www.analyticsvidhya.com/blog/2019/02/stanfordnlp-nlp-library-python/'''\n",
    "class StanfordNLP:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nlp = stanfordnlp.Pipeline()\n",
    "    \n",
    "    def word_tokenize(self, sentence):\n",
    "        doc = self.nlp(sentence)\n",
    "        wt = []\n",
    "        for sent in doc.sentences:\n",
    "            for wrd in sent.words:\n",
    "                wt.append(wrd.text)\n",
    "        return wt\n",
    "\n",
    "    def lemmatize(self, sentence):\n",
    "        doc = self.nlp(sentence)\n",
    "        wl = []\n",
    "        for sent in doc.sentences:\n",
    "            for wrd in sent.words:\n",
    "                wl.append(wrd.lemma)\n",
    "        return wl\n",
    "\n",
    "    def pos(self, sentence):\n",
    "        doc = self.nlp(sentence)\n",
    "        wp = []\n",
    "        for sent in doc.sentences:\n",
    "            for wrd in sent.words:\n",
    "                wp.append(wrd.pos)\n",
    "        return wp\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    sNLP = StanfordNLP()\n",
    "    text = 'he will be her in china.'\n",
    "    tree1 = sNLP.word_tokenize(text)\n",
    "    tree2 = sNLP.lemmatize(text)\n",
    "    tree3 = sNLP.pos(text)\n",
    "    print(\"word_tokenize:\", tree1)\n",
    "    print(\"lemmatize:\", tree2)\n",
    "    print(\"POS:\", tree3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded word lists\n",
    "import sys\n",
    "import numpy\n",
    "import nltk\n",
    "import collections\n",
    "import json\n",
    "\n",
    "yesnowords = [\"can\", \"could\", \"would\", \"is\", \"does\", \"has\", \"was\", \"were\", \"had\", \"have\", \"did\", \"are\", \"will\"]\n",
    "commonwords = [\"the\", \"a\", \"an\", \"is\", \"are\", \"were\", \".\"]\n",
    "questionwords = [\"who\", \"what\", \"where\", \"when\", \"why\", \"how\", \"whose\", \"which\", \"whom\"]\n",
    "\n",
    "\n",
    "# Take in a tokenized question and return the question type and body\n",
    "def processquestion(qwords):\n",
    "    \n",
    "    # Find \"question word\" (what, who, where, etc.)\n",
    "    questionword = \"\"\n",
    "    qidx = -1\n",
    "    \n",
    "    qw = [wo.lower() for wo in qwords if wo.lower() in questionwords]\n",
    "    if len(qw)==0:\n",
    "        return (\"YESNO\", qwords[1:])\n",
    "\n",
    "    for (idx, word) in enumerate(qwords):\n",
    "        if word.lower() in questionwords:\n",
    "            questionword = word.lower()\n",
    "            qidx = idx\n",
    "            break\n",
    "\n",
    "    if qidx < 0:\n",
    "        return (\"MISC\", qwords)\n",
    "\n",
    "    target = qwords[:qidx]+qwords[qidx+1:]\n",
    "    \n",
    "    type = \"MISC\"\n",
    "\n",
    "    # Determine question type\n",
    "    if questionword in [\"who\", \"whose\", \"whom\"]:\n",
    "        type = \"PERSON\"\n",
    "    elif questionword == \"where\":\n",
    "        type = \"PLACE\"\n",
    "    elif questionword == \"when\":\n",
    "        type = \"TIME\"\n",
    "    elif questionword == \"which\":\n",
    "        type = \"ITEM\"\n",
    "    elif questionword == \"how\":\n",
    "        if target[0] in [\"few\", \"little\", \"much\", \"many\"]:\n",
    "            type = \"QUANTITY\"\n",
    "            target = target[1:]\n",
    "        elif target[0] in [\"young\", \"old\", \"long\"]:\n",
    "            type = \"TIME\"\n",
    "            target = target[1:]\n",
    "\n",
    "    # Trim possible extra helper verb\n",
    "    if target[0] in yesnowords:\n",
    "        target = target[1:]\n",
    "    \n",
    "    # Return question data\n",
    "    return (type, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FACTOID', 'MISC', ['Nalanda', 'famous', 'for', 'the', 'production', 'of', 'sweet']]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"why is Nalanda famous for the production of sweet\"\n",
    "from nltk.tokenize import word_tokenize\n",
    "qwords  = word_tokenize(sentence)\n",
    "(type, target) = processquestion(qwords)\n",
    "if type=='YESNO':\n",
    "    print([type, target])\n",
    "else:\n",
    "    qDetail = ['FACTOID']\n",
    "    qDetail=qDetail+[type, target]\n",
    "    print(qDetail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Rajasthan food is influenced by the which community ?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-acc46ab02c27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Question: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mtree1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msNLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mtree2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msNLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mtree3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msNLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mtree4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msNLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqType_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-acc46ab02c27>\u001b[0m in \u001b[0;36mner\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructure_ne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnltk_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqType_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-acc46ab02c27>\u001b[0m in \u001b[0;36mstructure_ne\u001b[0;34m(self, ne_tree)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msubtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mne_tree\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtree\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# If subtree is a noun chunk, i.e. NE != \"O\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mne_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mne_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tree import Tree\n",
    "from qType import processquestion\n",
    "stopword = set(stopwords.words('english'))\n",
    "stopword.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "\n",
    "\n",
    "class QpreProcessing:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def keywords(self, sentence):\n",
    "        '''removes stop words too'''\n",
    "        doc = [i for i in word_tokenize(sentence) if i not in stopword]\n",
    "        return doc\n",
    "    \n",
    "    # NLTK POS and NER taggers   \n",
    "    def nltk_tagger(self, token_text):\n",
    "        tagged_words = nltk.pos_tag(token_text)\n",
    "        ne_tagged = nltk.ne_chunk(tagged_words)\n",
    "        return(ne_tagged)\n",
    "    \n",
    "    # Parse named entities from tree\n",
    "    def structure_ne(self, ne_tree):\n",
    "        ne = []\n",
    "        for subtree in ne_tree:\n",
    "            if type(subtree) == Tree: # If subtree is a noun chunk, i.e. NE != \"O\"\n",
    "                ne_label = subtree.label()\n",
    "                ne_string = \" \".join([token for token, pos in subtree.leaves()])\n",
    "                ne.append((ne_string, ne_label))\n",
    "        return ne\n",
    "\n",
    "    def pos(self, sentence):\n",
    "        sent = nltk.word_tokenize(sentence)\n",
    "        sent = nltk.pos_tag(sent)\n",
    "        return sent\n",
    "    \n",
    "    def ner(self, sentence):\n",
    "        return self.structure_ne(self.nltk_tagger(word_tokenize(sentence)))\n",
    "    \n",
    "    def qType_words(self, sentence):\n",
    "        qwords  = word_tokenize(sentence)\n",
    "        (type, target) = processquestion(qwords)\n",
    "        target = str(\" \".join(map(str, target))) \n",
    "        if type=='YESNO':\n",
    "            return [type, target]\n",
    "        else:\n",
    "            qDetail = ['FACTOID']\n",
    "            qDetail=qDetail+[type, target]\n",
    "            return qDetail\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    sNLP = QpreProcessing()\n",
    "    text = str(input('Question: '))\n",
    "    tree1 = sNLP.keywords(text)\n",
    "    tree2 = sNLP.ner(text)\n",
    "    tree3 = sNLP.pos(text)\n",
    "    tree4 = sNLP.qType_words(text)\n",
    "    print(\"tokenize:\", tree1)\n",
    "    print(\"ner:\", tree2)\n",
    "    print(\"POS:\", tree3)\n",
    "    print(\"qdetail:\", tree4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Rajasthan food is influenced by the which community ?\n",
      "{'Question': 'Rajasthan food is influenced by the which community ?', 'tokenize': ['Rajasthan', 'food', 'influenced', 'community'], 'ner': [('Rajasthan', 'GPE')], 'pos': [('Rajasthan', 'NNP'), ('food', 'NN'), ('is', 'VBZ'), ('influenced', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('which', 'WDT'), ('community', 'NN')], 'qdetail': ['FACTOID', 'ITEM', 'Rajasthan food is influenced by the community']}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tree import Tree\n",
    "from qType import processquestion\n",
    "import json\n",
    "stopword = set(stopwords.words('english'))\n",
    "stopword.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "\n",
    "\n",
    "class QpreProcessing:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def keywords(self, sentence):\n",
    "        '''removes stop words too'''\n",
    "        doc = [i for i in word_tokenize(sentence) if i not in stopword]\n",
    "        return doc\n",
    "    \n",
    "    # NLTK POS and NER taggers   \n",
    "    def nltk_tagger(self, token_text):\n",
    "        tagged_words = nltk.pos_tag(token_text)\n",
    "        ne_tagged = nltk.ne_chunk(tagged_words)\n",
    "        return(ne_tagged)\n",
    "    \n",
    "    # Parse named entities from tree\n",
    "    def structure_ne(self, ne_tree):\n",
    "        ne = []\n",
    "        for subtree in ne_tree:\n",
    "            if type(subtree) == Tree: # If subtree is a noun chunk, i.e. NE != \"O\"\n",
    "                ne_label = subtree.label()\n",
    "                ne_string = \" \".join([token for token, pos in subtree.leaves()])\n",
    "                ne.append((ne_string, ne_label))\n",
    "        return ne\n",
    "\n",
    "    def pos(self, sentence):\n",
    "        sent = nltk.word_tokenize(sentence)\n",
    "        sent = nltk.pos_tag(sent)\n",
    "        return sent\n",
    "    \n",
    "    def ner(self, sentence):\n",
    "        return self.structure_ne(self.nltk_tagger(word_tokenize(sentence)))\n",
    "    \n",
    "    def qType_words(self, sentence):\n",
    "        qwords  = word_tokenize(sentence)\n",
    "        (type, target) = processquestion(qwords)\n",
    "        target = str(\" \".join(map(str, target)))\n",
    "        if type=='YESNO':\n",
    "            return [type, target]\n",
    "        else:\n",
    "            qDetail = ['FACTOID']\n",
    "            qDetail=qDetail+[type, target]\n",
    "            return qDetail\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    sNLP = QpreProcessing()\n",
    "    text1 = str(input('Question: '))\n",
    "    text = text1.replace('?','')\n",
    "    qpResult = {'Question':text1}\n",
    "    \n",
    "    qpResult['tokenize'] = sNLP.keywords(text)\n",
    "    qpResult['ner'] = sNLP.ner(text)\n",
    "    qpResult['pos'] = sNLP.pos(text)\n",
    "    qpResult['qdetail'] = sNLP.qType_words(text)\n",
    "\n",
    "    with open('qpResult.json', 'w') as fp:\n",
    "        json.dump(qpResult, fp)\n",
    "    \n",
    "    print(qpResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Due to being split between Bangladesh and India, the cuisine of Bengal differs in the use of religiously significant items, as well as international cuisine, such as Chinese food from the diaspora, Portuguese items, and Anglo items from the colonial period.', 'Bangladesh generally does not have the same amount of access to global trade and therefore, food.', 'Mughal cuisine is a universal influencer in the Bengali palate, and has introduced Persian and Islamic foods to the region, as well as a number of more elaborate methods of preparing food, like marination using ghee.', 'Fish, rice, milk, and sugar all play crucial parts in Bengali cuisine.', 'Bengali cuisine can be subdivided into four different types of dishes, charbya , or food that is chewed, such as rice or fish; choáąŁya, or food that is sucked, such as ambal and tak; lehya , or foods that are meant to be licked, like chuttney; and peya, which includes drinks, mainly milk.', 'During the 19th century, many Odia-speaking cooks were employed in Bengal, which led to the transfer of several food items between the two regions.', 'Bengali cuisine is the only traditionally developed multi-course tradition from the Indian subcontinent that is analogous in structure to the modern service Ă\\xa0 la russe style of French cuisine, with food served course-wise rather than all at once.', 'Bengali cuisine differs according to regional tastes, such as the emphasis on the use of chilli pepper in the Chittagong district of Bangladesh However, across all its varieties, there is predominant use of mustard oil along with large amounts of spices.', 'The cuisine is known for subtle flavours with an emphasis on fish, meat, vegetables, lentils, and rice.', 'Bread is not a common dish in Bengali cuisine, but a deep fried version called luchi is popular.', 'Fresh sweetwater fish is one of its most distinctive features.', 'Bengalis prepare fish in many ways, such as steaming, braising, or stewing in vegetables and sauces based on coconut milk or mustard.', 'East Bengali food, which has a high presence in West Bengal and Bangladesh, is much spicier than the West Bengali cuisine, and tends to use high amounts of chilli, and is one of the spiciest cuisines in India and the World.', 'Shondesh and Rasgulla are popular sweet dishes made of sweetened, finely ground fresh cheese.', 'Rasgulla consists of paneer balls dipped in sugar syrup.', 'The \"Jaggery Rasgullas\" are even more famous.', 'The rasgulla originated in Bengal and later became popular in erstwhile Odisha.', 'The government of west Bengal has recently acquired the GI status of rasgulla after citing proof in court.', 'The cuisine is also found in the state of Tripura and the Barak Valley of Assam.Bihari cuisine may include litti chokha, a baked salted wheat-flour cake filled with sattu (baked chickpea flour) and some special spices, which is served with baigan bharta, made of roasted eggplant (brinjal) and tomatoes.', 'Among meat dishes, meat saalan is a popular dish made of mutton or goat curry with cubed potatoes in garam masala.', 'Dalpuri is another popular dish in Bihar.', 'It is salted wheat-flour bread, filled with boiled, crushed, and fried gram pulses.', 'Malpua is a popular sweet dish of Bihar, prepared by a mixture of maida, milk, bananas, cashew nuts, peanuts, raisins, sugar, water, and green cardamom.', 'Another notable sweet dish of Bihar is balushahi, which is prepared by a specially treated combination of maida and sugar along with ghee, and the other worldwide famous sweet, khaja, also very popular, is made from flour, vegetable fat, and sugar, which is mainly used in weddings and other occasions.', 'Silav near Nalanda is famous for its production.', 'During the festival of Chhath, thekua, a sweet dish made of ghee, jaggery, and whole-meal flour, flavoured with aniseed, is made.The food from Uttrakhand is known to be healthy and wholesome to suit the high-energy necessities of the cold, mountainous region.', 'It is a high protein diet that makes heavy use of pulses and vegetables.', 'Traditionally it is cooked over wood or charcoal fire mostly in iron utensils.', 'While also making use of condiments such as jeera, haldi and rai common in other Indian cuisines, Uttarakhand cuisine uses some exotic condiments like jambu, timmer, ghandhraini and bhangira.', 'Similarly, although the people in Uttarakhand also prepare the dishes common in other parts of northern India, several preparations are unique to Uttarakhand tradition such as rus, chudkani, dubuk, chadanji, jholi, kapa, etc.', 'Among dressed salads and sauces, kheere ka raita, nimbu mooli ka raita, daarim ki khatai and aam ka fajitha necessarily deserve a mention.', 'The cuisine mainly consists of food from two different sub regionsâ€”Garhwal and Kumaonâ€”though their basic ingredients are the same.', 'Both the Kumaoni and Garhwali styles make liberal use of ghee, lentils or pulses, vegetables and bhaat (rice).', 'They also use Badi (sun-dried Urad Dal balls) and Mungodi (sun-dried Moong Dal balls) as substitutes for vegetables at times.', 'During festivals and other celebrations, the people of Uttarakhand prepare special refreshments which include both salty preparations such as bada and sweet preparations such as pua and singal.', 'Uttarakhand also has several sweets (mithai) such as singodi, bal-mithai, malai laddu, etc.', 'native to its tradition.', 'Which Bengali sweet consists of paneer balls dipped in sugar syrup']\n"
     ]
    }
   ],
   "source": [
    "from fetch_final_docs import matching_score\n",
    "import nltk\n",
    "\n",
    "import os\n",
    "qu = \"Which Bengali sweet consists of paneer balls dipped in sugar syrup\"\n",
    "final_docs = matching_score(10, qu)\n",
    "#print(final_docs)\n",
    "answer_doc = \"\"\n",
    "for i in range(3):\n",
    "    file = open(final_docs[i], 'r', encoding='cp1250')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "    answer_doc+=text\n",
    "\n",
    "answer_doc=answer_doc.replace('\\n', ' ')\n",
    "answer_sent = nltk.sent_tokenize(answer_doc)\n",
    "answer_sent.append(qu)\n",
    "print(answer_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/a/27264462\n",
    "'''\n",
    "documents = (\n",
    "\"The sky is blue\",\n",
    "\"The sun is bright\",\n",
    "\"The sun in the sky is bright\",\n",
    "\"We can see the shining sun, the bright sun\"\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "t = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)\n",
    "print(t)\n",
    "'''\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer(min_df=1, stop_words=\"english\")                                                                                                                                                                                                   \n",
    "tfidf = vect.fit_transform(answer_sent)                                                                                                                                                                                                                       \n",
    "pairwise_similarity = tfidf * tfidf.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rasgulla consists of paneer balls dipped in sugar syrup.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np     \n",
    "\n",
    "arr = pairwise_similarity.toarray()     \n",
    "np.fill_diagonal(arr, np.nan)                                                                                                                                                                                                                            \n",
    "\n",
    "input_doc = qu                                                                                                                                                                                                 \n",
    "input_idx = answer_sent.index(input_doc)                                                                                                                                                                                                                      \n",
    "\n",
    "result_idx = np.nanargmax(arr[input_idx])                                                                                                                                                                                                                \n",
    "print(answer_sent[result_idx])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qu = \"Which Bengali sweet consists of paneer balls dipped in sugar syrup\"\n",
    "def get_answer(qu):\n",
    "    final_docs = matching_score(10, qu)\n",
    "    #print(final_docs)\n",
    "    answer_doc = \"\"\n",
    "    for i in range(3):\n",
    "        file = open(final_docs[i], 'r', encoding='cp1250')\n",
    "        text = file.read().strip()\n",
    "        file.close()\n",
    "        answer_doc+=text\n",
    "\n",
    "    answer_doc=answer_doc.replace('\\n', ' ')\n",
    "    answer_sent = nltk.sent_tokenize(answer_doc)\n",
    "    answer_sent.append(qu)\n",
    "    print(answer_sent)\n",
    "\n",
    "\n",
    "    vect = TfidfVectorizer(min_df=1, stop_words=\"english\")                                                                                                                                                                                                   \n",
    "    tfidf = vect.fit_transform(answer_sent)                                                                                                                                                                                                                       \n",
    "    pairwise_similarity = tfidf * tfidf.T \n",
    "\n",
    "    import numpy as np     \n",
    "\n",
    "    arr = pairwise_similarity.toarray()     \n",
    "    np.fill_diagonal(arr, np.nan)                                                                                                                                                                                                                            \n",
    "\n",
    "    input_doc = qu                                                                                                                                                                                                 \n",
    "    input_idx = answer_sent.index(input_doc)                                                                                                                                                                                                                      \n",
    "\n",
    "    result_idx = np.nanargmax(arr[input_idx])                                                                                                                                                                                                                \n",
    "    print(answer_sent[result_idx]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
